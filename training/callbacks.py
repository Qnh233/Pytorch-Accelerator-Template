import os
import torch
import numpy as np
from typing import Dict, Any, Optional
import json


class Callback:
    """å›è°ƒåŸºç±»"""

    def on_train_start(self, trainer):
        pass

    def on_train_end(self, trainer):
        pass

    def on_epoch_start(self, trainer):
        pass

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        pass

    def on_batch_start(self, trainer, batch):
        pass

    def on_batch_end(self, trainer, batch, outputs, loss):
        pass

    def get_logged_metrics(self, trainer):
        """è·å–å½“å‰è®°å½•çš„æŒ‡æ ‡"""
        if hasattr(trainer.accelerator, 'log_records'):
            return trainer.accelerator.log_records
        return {}
class ModelCheckpoint(Callback):
    """æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.save_dir = config["experiment"]["output_dir"]
        self.save_every = config["training"].get("save_every", 10)
        self.save_best = config["training"].get("save_best", True)
        self.monitor = config["training"].get("monitor_metric", "val_loss")
        self.mode = config["training"].get("monitor_mode", "min")  # "min" or "max"

        self.best_metric = None
        self.best_epoch = None

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(os.path.join(self.save_dir, "checkpoints"), exist_ok=True)

    def on_train_start(self, trainer):
        print(f"ModelCheckpoint: æ£€æŸ¥ç‚¹å°†ä¿å­˜åˆ° {self.save_dir}")

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        current_metric = val_metrics.get(self.monitor, train_metrics.get(self.monitor))

        if current_metric is None:
            print(f"Warning: ç›‘æ§æŒ‡æ ‡ {self.monitor} ä¸å­˜åœ¨")
            return

        # å®šæœŸä¿å­˜
        if trainer.epoch % self.save_every == 0:
            self._save_checkpoint(trainer, f"epoch_{trainer.epoch}", train_metrics, val_metrics)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if self.save_best:
            is_better = False
            if self.best_metric is None:
                is_better = True
            elif self.mode == "min" and current_metric < self.best_metric:
                is_better = True
            elif self.mode == "max" and current_metric > self.best_metric:
                is_better = True

            if is_better:
                self.best_metric = current_metric
                self.best_epoch = trainer.epoch
                self._save_checkpoint(trainer, f"best_model_{current_metric:.4f}", train_metrics, val_metrics)

                if trainer.accelerator.is_main_process:
                    print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! {self.monitor}: {current_metric:.4f} (epoch {trainer.epoch})")

    def _save_checkpoint(self, trainer, name: str, train_metrics: Dict[str, Any], val_metrics: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(self.save_dir, "checkpoints", f"{name}.pt")

        # å‡†å¤‡æ£€æŸ¥ç‚¹æ•°æ®
        checkpoint = {
            'epoch': trainer.epoch,
            'global_step': trainer.global_step,
            'model_state_dict': trainer.accelerator.unwrap_model(trainer.model).state_dict(),
            'optimizer_state_dict': trainer.optimizer.state_dict(),
            'metrics': {**train_metrics, **val_metrics} if hasattr(self, '_last_metrics') else {},
        }

        if trainer.scheduler is not None:
            checkpoint['scheduler_state_dict'] = trainer.scheduler.state_dict()

        # ä¿å­˜æ£€æŸ¥ç‚¹
        trainer.accelerator.save(checkpoint, checkpoint_path)

        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        info = {
            'epoch': trainer.epoch,
            'global_step': trainer.global_step,
            'best_metric': self.best_metric,
            'best_epoch': self.best_epoch,
            'config': trainer.config
        }

        with open(os.path.join(self.save_dir, "checkpoints", f"{name}_info.json"), 'w') as f:
            json.dump(info, f, indent=2)


class EarlyStopping(Callback):
    """æ—©åœå›è°ƒ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.patience = config["training"].get("early_stopping_patience", 10)
        self.monitor = config["training"].get("monitor_metric", "val_loss")
        self.mode = config["training"].get("monitor_mode", "min")
        self.min_delta = config["training"].get("min_delta", 0.0)

        self.best_metric = None
        self.counter = 0
        self.should_stop = False

    def on_train_start(self, trainer):
        print(f"EarlyStopping: ç›‘æ§æŒ‡æ ‡ {self.monitor}, è€å¿ƒå€¼ {self.patience}")

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        if self.should_stop:
            return True  # åœæ­¢è®­ç»ƒ

        current_metric = val_metrics.get(self.monitor, train_metrics.get(self.monitor))

        if current_metric is None:
            print(f"Warning: æ—©åœç›‘æ§æŒ‡æ ‡ {self.monitor} ä¸å­˜åœ¨")
            return False

        # æ£€æŸ¥æ˜¯å¦æ”¹å–„
        improved = False
        if self.best_metric is None:
            improved = True
        elif self.mode == "min" and current_metric < self.best_metric - self.min_delta:
            improved = True
        elif self.mode == "max" and current_metric > self.best_metric + self.min_delta:
            improved = True

        if improved:
            self.best_metric = current_metric
            self.counter = 0
            if trainer.accelerator.is_main_process:
                print(f"âœ… æŒ‡æ ‡æ”¹å–„: {self.monitor} = {current_metric:.4f}")
        else:
            self.counter += 1
            if trainer.accelerator.is_main_process:
                print(
                    f"â³ æ—©åœè®¡æ•°: {self.counter}/{self.patience}, {self.monitor} = {current_metric:.4f} (æœ€ä½³: {self.best_metric:.4f})")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
        if self.counter >= self.patience:
            self.should_stop = True
            if trainer.accelerator.is_main_process:
                print(f"ğŸ›‘ æ—©åœè§¦å‘! åœ¨ epoch {trainer.epoch} åœæ­¢è®­ç»ƒ")
            return True  # åœæ­¢è®­ç»ƒ

        return False


class LearningRateMonitor(Callback):
    """å­¦ä¹ ç‡ç›‘æ§å›è°ƒ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logging_interval = config["training"].get("lr_log_interval", "epoch")  # "epoch" or "step"
        self.step_interval = config["training"].get("lr_log_step_interval", 100)

    def on_batch_end(self, trainer, batch, outputs, loss):
        if self.logging_interval == "step" and trainer.global_step % self.step_interval == 0:
            lr = self._get_current_lr(trainer)
            if lr is not None:
                trainer.accelerator.log({"learning_rate": lr}, step=trainer.global_step)

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        if self.logging_interval == "epoch":
            lr = self._get_current_lr(trainer)
            if lr is not None:
                trainer.accelerator.log({"learning_rate": lr}, step=trainer.epoch)

                if trainer.accelerator.is_main_process:
                    print(f"ğŸ“Š å­¦ä¹ ç‡: {lr:.2e}")

    def _get_current_lr(self, trainer):
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        if trainer.optimizer is None:
            return None

        # è·å–ç¬¬ä¸€ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡
        for param_group in trainer.optimizer.param_groups:
            return param_group.get('lr', None)

        return None


class ProgressLogger(Callback):
    """è®­ç»ƒè¿›åº¦æ—¥å¿—å›è°ƒ"""
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.log_interval = config["training"].get("log_interval", 50)

    def on_train_start(self, trainer):
        if trainer.accelerator.is_main_process:
            print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {trainer.config['experiment']['output_dir']}")
            print(f"ğŸ“Š æ€»è½®æ•°: {trainer.config['training']['epochs']}")

    def on_epoch_start(self, trainer):
        if trainer.accelerator.is_main_process:
            print(f"\nğŸ“… Epoch {trainer.epoch}/{trainer.config['training']['epochs']}")

    def on_batch_end(self, trainer, batch, outputs, loss):
        if trainer.accelerator.is_main_process and trainer.global_step % self.log_interval == 0:
            print(f"   Step {trainer.global_step}, Loss: {loss.item():.4f}")

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        if trainer.accelerator.is_main_process:
            metrics_str = []
            train_loss = train_metrics.get("train_loss", 0)
            val_loss = val_metrics.get("val_loss", 0)
            metrics_str.append(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            metrics_str.append(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"   ğŸ“ˆ æŒ‡æ ‡: {', '.join(metrics_str)}")


class MetricsCallback(Callback):

    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config  # å•ç‹¬ä¿å­˜é…ç½®

    def on_epoch_end(self, trainer, train_metrics, val_metrics):
        """åœ¨epochç»“æŸæ—¶è®¡ç®—å„ç±»æŒ‡æ ‡"""
        if "outputs" not in val_metrics or "labels" not in val_metrics:
            return False

        outputs = val_metrics["outputs"]
        labels = val_metrics["labels"]

        # è®¡ç®—å‡†ç¡®ç‡
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        val_metrics["val_accuracy"] = correct / len(labels)
        print(f"   ğŸ§® è®¡ç®—éªŒè¯é›†å‡†ç¡®ç‡val_accuracy: {val_metrics['val_accuracy']:.4f}")
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡...

        # è®°å½•æ‰€æœ‰æŒ‡æ ‡
        if trainer.accelerator.is_main_process:
            trainer.accelerator.log(val_metrics)

        return False


# å›è°ƒå‡½æ•°æ³¨å†Œè¡¨
def get_callback(name: str):
    callback_registry = {
        "ModelCheckpoint": ModelCheckpoint,
        "EarlyStopping": EarlyStopping,
        "LearningRateMonitor": LearningRateMonitor,
        "ProgressLogger": ProgressLogger,
        "MetricsCallback": MetricsCallback,
    }


    if name not in callback_registry:
        raise ValueError(f"Unknown callback: {name}. Available: {list(callback_registry.keys())}")

    return callback_registry[name]
